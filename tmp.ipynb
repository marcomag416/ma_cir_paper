{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71b49f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(clip_model_name='B32', mixed_precision='fp16', cache_dir='.cache')\n",
      "Building CLIP model: openai/clip-vit-base-patch32\n",
      "Base model's number of parameters:  151277313\n",
      "LoRA model's number of parameters:  2671608\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "from models import build_clip, TwoEncoderVLM\n",
    "from peft import LoraModel, LoraConfig\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "vision_model, image_processor, text_model, tokenizer = build_clip(SimpleNamespace(\n",
    "    clip_model_name=\"B32\",      # one of: base32, base, large, huge, giga, meta-large, meta-huge\n",
    "    mixed_precision=\"fp16\",      # or \"fp32\"\n",
    "    cache_dir=\".cache\"\n",
    "))\n",
    "\n",
    "model = TwoEncoderVLM(\n",
    "    vision_model=vision_model,\n",
    "    text_model=text_model,\n",
    "    logit_scale=0.01,\n",
    "    trainable_temp=True,\n",
    "    proj_dim=512,\n",
    ")\n",
    "\n",
    "print(\"Base model's number of parameters: \", count_trainable_parameters(model))\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=\"FEATURE_EXTRACTION\",\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.01,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\", \"text_projection\", \"visual_projection\", \"position_embedding\", \"token_embedding\", \"patch_embedding\"],\n",
    ")\n",
    "\n",
    "model_lora = LoraModel(model, config, \"lora_adapter\")\n",
    "print(\"LoRA model's number of parameters: \", count_trainable_parameters(model_lora))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c86a440a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwoEncoderVLM(\n",
       "  (vision): CLIPVisionModelWithProjection(\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): lora.Conv2d(\n",
       "          (base_layer): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (lora_adapter): Conv2d(3, 8, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (lora_adapter): Conv2d(8, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "        (position_embedding): lora.Embedding(\n",
       "          (base_layer): Embedding(50, 768)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict()\n",
       "          (lora_B): ModuleDict()\n",
       "          (lora_embedding_A): ParameterDict(  (lora_adapter): Parameter containing: [torch.HalfTensor of size 8x50])\n",
       "          (lora_embedding_B): ParameterDict(  (lora_adapter): Parameter containing: [torch.HalfTensor of size 768x8])\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (out_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (visual_projection): lora.Linear(\n",
       "      (base_layer): Linear(in_features=768, out_features=512, bias=False)\n",
       "      (lora_dropout): ModuleDict(\n",
       "        (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "      )\n",
       "      (lora_A): ModuleDict(\n",
       "        (lora_adapter): Linear(in_features=768, out_features=8, bias=False)\n",
       "      )\n",
       "      (lora_B): ModuleDict(\n",
       "        (lora_adapter): Linear(in_features=8, out_features=512, bias=False)\n",
       "      )\n",
       "      (lora_embedding_A): ParameterDict()\n",
       "      (lora_embedding_B): ParameterDict()\n",
       "      (lora_magnitude_vector): ModuleDict()\n",
       "    )\n",
       "  )\n",
       "  (text): CLIPTextModelWithProjection(\n",
       "    (text_model): CLIPTextTransformer(\n",
       "      (embeddings): CLIPTextEmbeddings(\n",
       "        (token_embedding): lora.Embedding(\n",
       "          (base_layer): Embedding(49408, 512)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict()\n",
       "          (lora_B): ModuleDict()\n",
       "          (lora_embedding_A): ParameterDict(  (lora_adapter): Parameter containing: [torch.HalfTensor of size 8x49408])\n",
       "          (lora_embedding_B): ParameterDict(  (lora_adapter): Parameter containing: [torch.HalfTensor of size 512x8])\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "        (position_embedding): lora.Embedding(\n",
       "          (base_layer): Embedding(77, 512)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict()\n",
       "          (lora_B): ModuleDict()\n",
       "          (lora_embedding_A): ParameterDict(  (lora_adapter): Parameter containing: [torch.HalfTensor of size 8x77])\n",
       "          (lora_embedding_B): ParameterDict(  (lora_adapter): Parameter containing: [torch.HalfTensor of size 512x8])\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=512, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=512, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=512, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (out_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=512, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=512, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (text_projection): lora.Linear(\n",
       "      (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "      (lora_dropout): ModuleDict(\n",
       "        (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "      )\n",
       "      (lora_A): ModuleDict(\n",
       "        (lora_adapter): Linear(in_features=512, out_features=8, bias=False)\n",
       "      )\n",
       "      (lora_B): ModuleDict(\n",
       "        (lora_adapter): Linear(in_features=8, out_features=512, bias=False)\n",
       "      )\n",
       "      (lora_embedding_A): ParameterDict()\n",
       "      (lora_embedding_B): ParameterDict()\n",
       "      (lora_magnitude_vector): ModuleDict()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "024abc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'datasets.mscoco' in sys.modules:\n",
    "    del sys.modules['datasets.mscoco']\n",
    "if 'datasets' in sys.modules:\n",
    "    del sys.modules['datasets']\n",
    "\n",
    "from datasets.mscoco import MSCOCOCaptions\n",
    "\n",
    "from datasets.mscoco import MSCOCOCaptions\n",
    "\n",
    "train_dataset = MSCOCOCaptions(\n",
    "    root=\"data/mscoco/images/train2017\",\n",
    "    annotations_file=\"data/mscoco/annotations/captions_train2017.json\",\n",
    "    image_transform=image_processor,\n",
    "    caption_transform=tokenizer,\n",
    ")\n",
    "\n",
    "eval_dataset = MSCOCOCaptions(\n",
    "    root=\"data/mscoco/images/val2017\",\n",
    "    annotations_file=\"data/mscoco/annotations/captions_val2017.json\",\n",
    "    image_transform=image_processor,\n",
    "    caption_transform=tokenizer,\n",
    "    resize_dataset=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c783d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def loss_fn(outputs, inputs, num_items_in_batch, temperature=0.07, **kwargs):\n",
    "    vision_embeds = outputs['vision_embeds']\n",
    "    text_embeds = outputs['text_embeds']\n",
    "    batch_size = vision_embeds.size(0)\n",
    "    logits = (vision_embeds @ text_embeds.t()) / temperature\n",
    "    labels = torch.arange(batch_size).to(vision_embeds.device)\n",
    "    loss_i2t = torch.nn.functional.cross_entropy(logits, labels)\n",
    "    loss_t2i = torch.nn.functional.cross_entropy(logits.t(), labels)\n",
    "    loss = (loss_i2t + loss_t2i) / 2\n",
    "    return loss\n",
    "\n",
    "def loss_fn_debug(*args, **kwargs):\n",
    "    print(\"Debug: loss_fn_debug called with args:\", len(args), \"and kwargs:\", kwargs)\n",
    "    return loss_fn(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f8836fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(4, 512)\n",
    "y = x.mean(dim=1)\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12479ade",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_constant_schedule' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlosses\u001b[39;00m\n\u001b[32m      9\u001b[39m loss_fn = losses.build_loss_fn(\u001b[33m\"\u001b[39m\u001b[33mma_bi_sw\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m train.train(SimpleNamespace(\n\u001b[32m     13\u001b[39m     train_dataset=train_dataset,\n\u001b[32m     14\u001b[39m     eval_dataset=eval_dataset,\n\u001b[32m     15\u001b[39m     data_collator=train_dataset.collate_fn,\n\u001b[32m     16\u001b[39m     model=model,\n\u001b[32m     17\u001b[39m     loss_fn=loss_fn,\n\u001b[32m     18\u001b[39m     batch_size=\u001b[32m2\u001b[39m,\n\u001b[32m     19\u001b[39m     num_epochs=\u001b[32m1\u001b[39m,\n\u001b[32m     20\u001b[39m     lr=\u001b[32m1e-4\u001b[39m,\n\u001b[32m     21\u001b[39m     warmup_ratio=\u001b[32m0.1\u001b[39m,\n\u001b[32m     22\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33mcheckpoints\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     23\u001b[39m     tqdm=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     24\u001b[39m     logging_steps=\u001b[32m2\u001b[39m,\n\u001b[32m     25\u001b[39m     save_steps=\u001b[32m2\u001b[39m,\n\u001b[32m     26\u001b[39m     save_strategy=\u001b[33m\"\u001b[39m\u001b[33msteps\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m     debug=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     28\u001b[39m     max_steps=\u001b[32m4\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     scheduler=\u001b[43mget_constant_schedule\u001b[49m,\n\u001b[32m     30\u001b[39m ))\n",
      "\u001b[31mNameError\u001b[39m: name 'get_constant_schedule' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import get_constant_schedule\n",
    "\n",
    "\n",
    "if 'train' in sys.modules:\n",
    "    del sys.modules['train']\n",
    "import train\n",
    "\n",
    "if 'losses' in sys.modules:\n",
    "    del sys.modules[\"losses\"]\n",
    "import losses\n",
    "\n",
    "loss_fn = losses.build_loss_fn(\"ma_bi_sw\")\n",
    "\n",
    "\n",
    "train.train(SimpleNamespace(\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=train_dataset.collate_fn,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    batch_size=2,\n",
    "    num_epochs=1,\n",
    "    lr=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    output_dir=\"checkpoints\",\n",
    "    tqdm=True,\n",
    "    logging_steps=2,\n",
    "    save_steps=2,\n",
    "    save_strategy=\"steps\",\n",
    "    debug=True,\n",
    "    max_steps=4,\n",
    "    scheduler=get_constant_schedule,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec193165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(4, 512)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "n =  x.norm(dim=1, keepdim=True)\n",
    "\n",
    "x = x / n\n",
    "\n",
    "print(n.shape)\n",
    "print(x.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma_cir_paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
