{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b49f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/magnanini/ma_cir_paper/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(clip_model_name='B32', mixed_precision='fp16', cache_dir='.cache')\n",
      "Building CLIP model: openai/clip-vit-base-patch32\n",
      "Base model's number of parameters:  151277313\n",
      "LoRA model's number of parameters:  2671608\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "from models import build_clip, TwoEncoderVLM\n",
    "from peft import LoraModel, LoraConfig\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "vision_model, image_processor, text_model, tokenizer = build_clip(SimpleNamespace(\n",
    "    clip_model_name=\"B32\",      # one of: base32, base, large, huge, giga, meta-large, meta-huge\n",
    "    mixed_precision=\"fp16\",      # or \"fp32\"\n",
    "    cache_dir=\".cache\"\n",
    "))\n",
    "\n",
    "model = TwoEncoderVLM(\n",
    "    vision_model=vision_model,\n",
    "    text_model=text_model,\n",
    "    logit_scale=0.01,\n",
    "    trainable_temp=True,\n",
    "    proj_dim=512,\n",
    "    tokenizer=tokenizer,\n",
    "    image_processor=image_processor\n",
    ")\n",
    "\n",
    "print(\"Base model's number of parameters: \", count_trainable_parameters(model))\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=\"FEATURE_EXTRACTION\",\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.01,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\", \"text_projection\", \"visual_projection\", \"position_embedding\", \"token_embedding\", \"patch_embedding\"],\n",
    ")\n",
    "\n",
    "model_lora = LoraModel(model, config, \"lora_adapter\")\n",
    "print(\"LoRA model's number of parameters: \", count_trainable_parameters(model_lora))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c86a440a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwoEncoderVLM(\n",
       "  (vision): CLIPVisionModelWithProjection(\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): lora.Conv2d(\n",
       "          (base_layer): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (lora_adapter): Conv2d(3, 8, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (lora_adapter): Conv2d(8, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "        (position_embedding): lora.Embedding(\n",
       "          (base_layer): Embedding(50, 768)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict()\n",
       "          (lora_B): ModuleDict()\n",
       "          (lora_embedding_A): ParameterDict(  (lora_adapter): Parameter containing: [torch.HalfTensor of size 8x50])\n",
       "          (lora_embedding_B): ParameterDict(  (lora_adapter): Parameter containing: [torch.HalfTensor of size 768x8])\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (out_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (visual_projection): lora.Linear(\n",
       "      (base_layer): Linear(in_features=768, out_features=512, bias=False)\n",
       "      (lora_dropout): ModuleDict(\n",
       "        (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "      )\n",
       "      (lora_A): ModuleDict(\n",
       "        (lora_adapter): Linear(in_features=768, out_features=8, bias=False)\n",
       "      )\n",
       "      (lora_B): ModuleDict(\n",
       "        (lora_adapter): Linear(in_features=8, out_features=512, bias=False)\n",
       "      )\n",
       "      (lora_embedding_A): ParameterDict()\n",
       "      (lora_embedding_B): ParameterDict()\n",
       "      (lora_magnitude_vector): ModuleDict()\n",
       "    )\n",
       "  )\n",
       "  (text): CLIPTextModelWithProjection(\n",
       "    (text_model): CLIPTextTransformer(\n",
       "      (embeddings): CLIPTextEmbeddings(\n",
       "        (token_embedding): lora.Embedding(\n",
       "          (base_layer): Embedding(49408, 512)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict()\n",
       "          (lora_B): ModuleDict()\n",
       "          (lora_embedding_A): ParameterDict(  (lora_adapter): Parameter containing: [torch.HalfTensor of size 8x49408])\n",
       "          (lora_embedding_B): ParameterDict(  (lora_adapter): Parameter containing: [torch.HalfTensor of size 512x8])\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "        (position_embedding): lora.Embedding(\n",
       "          (base_layer): Embedding(77, 512)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict()\n",
       "          (lora_B): ModuleDict()\n",
       "          (lora_embedding_A): ParameterDict(  (lora_adapter): Parameter containing: [torch.HalfTensor of size 8x77])\n",
       "          (lora_embedding_B): ParameterDict(  (lora_adapter): Parameter containing: [torch.HalfTensor of size 512x8])\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=512, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=512, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=512, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (out_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=512, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=512, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (lora_adapter): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (text_projection): lora.Linear(\n",
       "      (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "      (lora_dropout): ModuleDict(\n",
       "        (lora_adapter): Dropout(p=0.01, inplace=False)\n",
       "      )\n",
       "      (lora_A): ModuleDict(\n",
       "        (lora_adapter): Linear(in_features=512, out_features=8, bias=False)\n",
       "      )\n",
       "      (lora_B): ModuleDict(\n",
       "        (lora_adapter): Linear(in_features=8, out_features=512, bias=False)\n",
       "      )\n",
       "      (lora_embedding_A): ParameterDict()\n",
       "      (lora_embedding_B): ParameterDict()\n",
       "      (lora_magnitude_vector): ModuleDict()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "024abc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'datasets.mscoco' in sys.modules:\n",
    "    del sys.modules['datasets.mscoco']\n",
    "if 'datasets' in sys.modules:\n",
    "    del sys.modules['datasets']\n",
    "\n",
    "from datasets.mscoco import MSCOCOCaptions\n",
    "\n",
    "from datasets.mscoco import MSCOCOCaptions\n",
    "\n",
    "train_dataset = MSCOCOCaptions(\n",
    "    root=\"data/mscoco/images/train2017\",\n",
    "    annotations_file=\"data/mscoco/annotations/captions_train2017.json\",\n",
    "    image_transform=image_processor,\n",
    "    caption_transform=tokenizer,\n",
    ")\n",
    "\n",
    "eval_dataset = MSCOCOCaptions(\n",
    "    root=\"data/mscoco/images/val2017\",\n",
    "    annotations_file=\"data/mscoco/annotations/captions_val2017.json\",\n",
    "    image_transform=image_processor,\n",
    "    caption_transform=tokenizer,\n",
    "    resize_dataset=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c783d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def loss_fn(outputs, inputs, num_items_in_batch, temperature=0.07, **kwargs):\n",
    "    vision_embeds = outputs['vision_embeds']\n",
    "    text_embeds = outputs['text_embeds']\n",
    "    batch_size = vision_embeds.size(0)\n",
    "    logits = (vision_embeds @ text_embeds.t()) / temperature\n",
    "    labels = torch.arange(batch_size).to(vision_embeds.device)\n",
    "    loss_i2t = torch.nn.functional.cross_entropy(logits, labels)\n",
    "    loss_t2i = torch.nn.functional.cross_entropy(logits.t(), labels)\n",
    "    loss = (loss_i2t + loss_t2i) / 2\n",
    "    return loss\n",
    "\n",
    "def loss_fn_debug(*args, **kwargs):\n",
    "    print(\"Debug: loss_fn_debug called with args:\", len(args), \"and kwargs:\", kwargs)\n",
    "    return loss_fn(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12479ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from /nfs/home/magnanini/.netrc.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcomag416\u001b[0m (\u001b[33mmarco-magnanini\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/magnanini/ma_cir_paper/wandb/run-20260127_150727-qxxrjnm5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marco-magnanini/vlm-post-train/runs/qxxrjnm5' target=\"_blank\">run</a></strong> to <a href='https://wandb.ai/marco-magnanini/vlm-post-train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marco-magnanini/vlm-post-train' target=\"_blank\">https://wandb.ai/marco-magnanini/vlm-post-train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marco-magnanini/vlm-post-train/runs/qxxrjnm5' target=\"_blank\">https://wandb.ai/marco-magnanini/vlm-post-train/runs/qxxrjnm5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/magnanini/ma_cir_paper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1118: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='None' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      None\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Modality Gap</th>\n",
       "      <th>Alignment</th>\n",
       "      <th>Xsc-sr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692032</td>\n",
       "      <td>0.948730</td>\n",
       "      <td>1.460938</td>\n",
       "      <td>1.281250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/nfs/home/magnanini/ma_cir_paper/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 358, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/nfs/home/magnanini/ma_cir_paper/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/nfs/home/magnanini/ma_cir_paper/datasets/macir.py\", line 116, in __getitem__\n    image = self.preprocess(im, return_tensors='pt')['pixel_values'][0]\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: 'NoneType' object is not callable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlosses\u001b[39;00m\n\u001b[32m     12\u001b[39m loss_fn = losses.build_loss_fn(\u001b[33m\"\u001b[39m\u001b[33mma_bi_sw\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSimpleNamespace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcheckpoints\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msteps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_constant_schedule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ma_cir_paper/train.py:247\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    205\u001b[39m training_args = TrainingArguments(\n\u001b[32m    206\u001b[39m \toutput_dir=output_dir,\n\u001b[32m    207\u001b[39m \tper_device_train_batch_size=get(\u001b[33m\"\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m128\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m    232\u001b[39m \tprediction_loss_only=\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[32m    233\u001b[39m )\n\u001b[32m    235\u001b[39m trainer = CustomLossTrainer(\n\u001b[32m    236\u001b[39m \tmodel=model,\n\u001b[32m    237\u001b[39m \targs=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m \toptimizers=(get(\u001b[33m\"\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), get(\u001b[33m\"\u001b[39m\u001b[33mlr_scheduler\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    245\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# Save final artifacts\u001b[39;00m\n\u001b[32m    250\u001b[39m trainer.save_model(output_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ma_cir_paper/.venv/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ma_cir_paper/.venv/lib/python3.12/site-packages/transformers/trainer.py:2576\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2573\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_train_begin(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.eval_on_start:\n\u001b[32m-> \u001b[39m\u001b[32m2576\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_scheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2578\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs_trained, num_train_epochs):\n\u001b[32m   2579\u001b[39m     epoch_dataloader = train_dataloader\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ma_cir_paper/.venv/lib/python3.12/site-packages/transformers/trainer.py:3170\u001b[39m, in \u001b[36mTrainer._evaluate\u001b[39m\u001b[34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[39m\n\u001b[32m   3169\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m3170\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3171\u001b[39m     \u001b[38;5;28mself\u001b[39m._report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m.state.global_step, metrics)\n\u001b[32m   3173\u001b[39m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ma_cir_paper/train.py:92\u001b[39m, in \u001b[36mCustomLossTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# 2. Run custom evaluation task if provided\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.custom_eval_func:\n\u001b[32m     88\u001b[39m     \u001b[38;5;66;03m# print(f\"\\n[Custom Eval] Running custom evaluation task...\")\u001b[39;00m\n\u001b[32m     89\u001b[39m     \n\u001b[32m     90\u001b[39m     \u001b[38;5;66;03m# Call the user function. We pass 'self' (the trainer) so it has access \u001b[39;00m\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# to self.model, self.accelerator, etc.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     custom_metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcustom_eval_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m     \u001b[38;5;66;03m# Prefix custom metrics to keep them clean in WandB (e.g., 'retrieval/Recall@1')\u001b[39;00m\n\u001b[32m     95\u001b[39m     \u001b[38;5;66;03m# You can handle prefixing inside custom_eval_func or here\u001b[39;00m\n\u001b[32m     96\u001b[39m     \n\u001b[32m     97\u001b[39m     \u001b[38;5;66;03m# Merge dicts\u001b[39;00m\n\u001b[32m     98\u001b[39m     metrics.update(custom_metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ma_cir_paper/train.py:136\u001b[39m, in \u001b[36mevaluate_on_tasks\u001b[39m\u001b[34m(trainer)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[33;03mEvaluate the model on custom tasks and return metrics.\u001b[39;00m\n\u001b[32m    129\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    133\u001b[39m \u001b[33;03m\tdict: Dictionary of evaluation metrics.\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    135\u001b[39m model = trainer.model\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m ma_cir_metrics = \u001b[43mevaluate_macir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m\t\u001b[49m\u001b[43meval_level\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfull_splits\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m\t\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataloader_num_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m metrics = {}\n\u001b[32m    144\u001b[39m metrics.update({\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mma-cir/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m ma_cir_metrics.items()})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ma_cir_paper/evals/ma_cir.py:339\u001b[39m, in \u001b[36mevaluate_macir\u001b[39m\u001b[34m(model, eval_level, split, batch_size, num_workers)\u001b[39m\n\u001b[32m    325\u001b[39m     macir_db = build_macir_dataset(\n\u001b[32m    326\u001b[39m \t\tsplit=split,\n\u001b[32m    327\u001b[39m \t\tmode=\u001b[33m\"\u001b[39m\u001b[33mdatabase\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    330\u001b[39m \t\tcaption_transform=model.tokenizer,\n\u001b[32m    331\u001b[39m \t)\n\u001b[32m    332\u001b[39m     ma_cir_query = build_macir_dataset(\n\u001b[32m    333\u001b[39m \t\tsplit=split,\n\u001b[32m    334\u001b[39m \t\tmode=\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    337\u001b[39m \t\tcaption_transform=model.tokenizer,\n\u001b[32m    338\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m     index_features, index_names = \u001b[43mmacir_generate_index_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclip_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmacir_db\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m     predicted_features, reference_names, target_names, composition_types, condition_types = macir_generate_test_predictions(\n\u001b[32m    347\u001b[39m         clip_model=model,\n\u001b[32m    348\u001b[39m         query_test_dataset=ma_cir_query,\n\u001b[32m   (...)\u001b[39m\u001b[32m    351\u001b[39m         num_workers=num_workers,\n\u001b[32m    352\u001b[39m     )\n\u001b[32m    353\u001b[39m     metrics = macir_compute_test_metrics(\n\u001b[32m    354\u001b[39m         predicted_features=predicted_features,\n\u001b[32m    355\u001b[39m         reference_names=reference_names,\n\u001b[32m   (...)\u001b[39m\u001b[32m    362\u001b[39m         split=split,\n\u001b[32m    363\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ma_cir_paper/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ma_cir_paper/evals/ma_cir.py:311\u001b[39m, in \u001b[36mmacir_generate_index_features\u001b[39m\u001b[34m(clip_model, index_dataset, batch_size, num_workers)\u001b[39m\n\u001b[32m    308\u001b[39m all_index_features = []\n\u001b[32m    309\u001b[39m all_index_names = []\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvision\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_names\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage_name\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ma_cir_paper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:741\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    739\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    740\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    744\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    745\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    746\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    747\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ma_cir_paper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1548\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1546\u001b[39m worker_id = \u001b[38;5;28mself\u001b[39m._task_info.pop(idx)[\u001b[32m0\u001b[39m]\n\u001b[32m   1547\u001b[39m \u001b[38;5;28mself\u001b[39m._rcvd_idx += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1548\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ma_cir_paper/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1586\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data, worker_idx)\u001b[39m\n\u001b[32m   1584\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1585\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1586\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1587\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ma_cir_paper/.venv/lib/python3.12/site-packages/torch/_utils.py:775\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    772\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[32m    773\u001b[39m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m775\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mTypeError\u001b[39m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/nfs/home/magnanini/ma_cir_paper/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 358, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/nfs/home/magnanini/ma_cir_paper/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/nfs/home/magnanini/ma_cir_paper/datasets/macir.py\", line 116, in __getitem__\n    image = self.preprocess(im, return_tensors='pt')['pixel_values'][0]\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: 'NoneType' object is not callable\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_constant_schedule\n",
    "\n",
    "\n",
    "if 'train' in sys.modules:\n",
    "    del sys.modules['train']\n",
    "import train\n",
    "\n",
    "if 'losses' in sys.modules:\n",
    "    del sys.modules[\"losses\"]\n",
    "import losses\n",
    "\n",
    "loss_fn = losses.build_loss_fn(\"ma_bi_sw\")\n",
    "\n",
    "\n",
    "train.train(SimpleNamespace(\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=train_dataset.collate_fn,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    batch_size=2,\n",
    "    num_epochs=1,\n",
    "    lr=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    output_dir=\"checkpoints\",\n",
    "    tqdm=True,\n",
    "    logging_steps=2,\n",
    "    save_steps=2,\n",
    "    save_strategy=\"steps\",\n",
    "    debug=True,\n",
    "    max_steps=4,\n",
    "    scheduler=get_constant_schedule,\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma_cir_paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
