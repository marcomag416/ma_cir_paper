# Adapted from https://github.com/miccunifi/CIRCO/blob/main/src/dataset.py

import json
from pathlib import Path
from typing import Union, List, Dict, Literal

import PIL
import PIL.Image
import torch.utils.data
import torchvision
from torch.utils.data import Dataset


class CIRCODataset(Dataset):
    """
    CIRCO dataset
    """

    def __init__(self, data_path: Union[str, Path], split: Literal['val', 'test'], mode: Literal['relative', 'classic'],
                 preprocess: callable, tokenizer: callable = None, max_length_tokenizer: int = 77):
        """
        Args:
            data_path (Union[str, Path]): path to CIRCO dataset
            split (str): dataset split, should be in ['test', 'val']
            mode (str): dataset mode, should be in ['relative', 'classic']
            preprocess (callable): function which preprocesses the image
            tokenizer (callable, optional): function which tokenizes the text. Defaults to None.
            max_length_tokenizer (int, optional): maximum length for tokenizer. Defaults to 77.
        """

        # Set dataset paths and configurations
        data_path = Path(data_path)
        self.mode = mode
        self.split = split
        self.preprocess = preprocess
        self.tokenizer = tokenizer
        self.max_length_tokenizer = max_length_tokenizer
        self.data_path = data_path

        # Ensure input arguments are valid
        if mode not in ['relative', 'classic']:
            raise ValueError("mode should be in ['relative', 'classic']")
        if split not in ['test', 'val']:
            raise ValueError("split should be in ['test', 'val']")

        # Load COCO images information
        with open(data_path / 'COCO2017_unlabeled' / "annotations" / "image_info_unlabeled2017.json", "r") as f:
            imgs_info = json.load(f)

        self.img_paths = [data_path / 'COCO2017_unlabeled' / "unlabeled2017" / img_info["file_name"] for img_info in
                          imgs_info["images"]]
        self.img_ids = [img_info["id"] for img_info in imgs_info["images"]]
        self.img_ids_indexes_map = {str(img_id): i for i, img_id in enumerate(self.img_ids)}

        # get CIRCO annotations
        with open(data_path / 'annotations' / f'{split}.json', "r") as f:
            self.annotations: List[dict] = json.load(f)

        # Get maximum number of ground truth images (for padding when loading the images)
        self.max_num_gts = 23  # Maximum number of ground truth images

        print(f"CIRCODataset {split} dataset in {mode} mode initialized")

    def get_target_img_ids(self, index) -> Dict[str, int]:
        """
        Returns the id of the target image and ground truth images for a given query

        Args:
            index (int): id of the query

        Returns:
             Dict[str, int]: dictionary containing target image id and a list of ground truth image ids
        """

        return {
            'target_img_id': self.annotations[index]['target_img_id'],
            'gt_img_ids': self.annotations[index]['gt_img_ids']
        }

    def get_semantic_aspects(self, index):
        """ Returns the semantic aspects for a given query"""
        return self.annotations[index].get('semantic_aspects', [])

    def __getitem__(self, index) -> dict:
        """
        Returns a specific item from the dataset based on the index.

        In 'classic' mode, the dataset yields a dictionary with the following keys: [img, img_id]
        In 'relative' mode, the dataset yields dictionaries with the following keys:
            - [reference_img, reference_img_id, target_img, target_img_id, relative_caption, shared_concept, gt_img_ids,
            query_id] if split == val
            - [reference_img, reference_img_id, relative_caption, shared_concept, query_id]  if split == test
        """

        if self.mode == 'relative':
            # Get the query id
            query_id = str(self.annotations[index]['id'])

            # Get relative caption and shared concept
            relative_caption = self.annotations[index]['relative_caption']
            shared_concept = self.annotations[index]['shared_concept']

            if self.tokenizer is not None:
                transformed_caption = self.tokenizer(relative_caption, 
                                                  padding='max_length',
                                                  max_length=self.max_length_tokenizer,
                                                  truncation=True,
                                                  return_tensors='pt')
            else:
                transformed_caption = {"input_ids": [None], "attention_mask": [None]}

            # Get the reference image
            reference_img_id = str(self.annotations[index]['reference_img_id'])
            reference_img_path = self.img_paths[self.img_ids_indexes_map[reference_img_id]]
            reference_img = self.preprocess(PIL.Image.open(reference_img_path), return_tensors='pt')['pixel_values'][0]

            if self.split == 'val':
                # Get the target image and ground truth images
                target_img_id = str(self.annotations[index]['target_img_id'])
                gt_img_ids = [str(x) for x in self.annotations[index]['gt_img_ids']]
                target_img_path = self.img_paths[self.img_ids_indexes_map[target_img_id]]
                target_img = self.preprocess(PIL.Image.open(target_img_path), return_tensors='pt')['pixel_values'][0]

                # Pad ground truth image IDs with zeros for collate_fn
                gt_img_ids += [''] * (self.max_num_gts - len(gt_img_ids))

                return {
                    'reference_img': reference_img,
                    'reference_imd_id': reference_img_id,
                    'target_img': target_img,
                    'target_img_id': target_img_id,
                    'relative_caption': relative_caption,
                    'input_ids': transformed_caption["input_ids"][0],
                    'attention_mask': transformed_caption["attention_mask"][0],
                    'shared_concept': shared_concept,
                    'gt_img_ids': gt_img_ids,
                    'query_id': query_id,
                }

            elif self.split == 'test':
                return {
                    'reference_img': reference_img,
                    'reference_imd_id': reference_img_id,
                    'relative_caption': relative_caption,
                    'input_ids': transformed_caption["input_ids"][0],
                    'attention_mask': transformed_caption["attention_mask"][0],
                    'shared_concept': shared_concept,
                    'query_id': query_id,
                }

        elif self.mode == 'classic':
            # Get image ID and image path
            img_id = str(self.img_ids[index])
            img_path = self.img_paths[index]

            # Preprocess image and return
            img = self.preprocess(PIL.Image.open(img_path), return_tensors='pt')['pixel_values'][0]
            return {
                'img': img,
                'img_id': img_id
            }

    def __len__(self):
        """
        Returns the length of the dataset.
        """
        if self.mode == 'relative':
            return len(self.annotations)
        elif self.mode == 'classic':
            return len(self.img_ids)
        else:
            raise ValueError("mode should be in ['relative', 'classic']")


if __name__ == '__main__':
    """
    Test the CIRCODataset class
    """
    from torchvision import transforms

    transform = torchvision.transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
    ])
    base_path = Path(__file__).absolute().parents[1].absolute()  # Getting the path to the base directory
    dataset = CIRCODataset(data_path=base_path, split='val', mode='relative', preprocess=transform)
    loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0)
    for batch in loader:
        for key, value in batch.items():
            if torch.is_tensor(value):
                print(f"{key} shape: {value.shape} dtype: {value.dtype} device: {value.device}")
            else:
                print(f"{key} length: {len(value)}")
            print("\n")


def build_circo_dataset(split: Literal['val', 'test'],
                         mode: Literal['relative', 'classic'],
                      preprocess: callable, 
                      tokenizer: callable = None, 
                      max_length_tokenizer: int = 77, 
                      data_path: Union[str, Path]= "data/circo"
) -> CIRCODataset:
    """
    Build CIRCO dataset

    Args:
        data_path (Union[str, Path]): path to CIRCO dataset
        split (str): dataset split, should be in ['test', 'val']
        mode (str): dataset mode, should be in ['relative', 'classic']
        preprocess (callable): function which preprocesses the image

    Returns:
        CIRCODataset: CIRCO dataset
    """
    return CIRCODataset(data_path=data_path, split=split, mode=mode, preprocess=preprocess, tokenizer=tokenizer, max_length_tokenizer=max_length_tokenizer)